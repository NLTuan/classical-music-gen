{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6331f45e-9ae5-429f-b0d4-2a9610da8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28fdf0fc-1e1e-4bfb-bcee-618934aff025",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 256\n",
    "block_size = 32\n",
    "bs= 16\n",
    "vocab_size = 500\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "504fd50a-c01a-4056-a0e1-8f1855b50d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.9/site-packages/miditok/tokenizations/remi.py:77: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting music files (chunks_train): 100%|██████████| 132/132 [00:11<00:00, 11.09it/s]\n",
      "Splitting music files (chunks_test): 100%|██████████| 5/5 [00:00<00:00, 11.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38007, 1475)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Creating a multitrack tokenizer, read the doc to explore all the parameters\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "directories = ['train', 'test']\n",
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "files_paths = list(Path(\"/notebooks/classical-music-gen/midis_train\").glob(\"**/*.midi\")), list(Path(\"/notebooks/classical-music-gen/midis_test\").glob(\"**/*.midi\"))\n",
    "\n",
    "tokenizer.train(vocab_size=vocab_size, files_paths=files_paths[0])\n",
    "tokenizer.save(Path(\"tokenizer\", \"tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "tokenizer.push_to_hub(\"ABicGrill/miditok_tokenizer\", private=True, token=\"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\")\n",
    "\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "for i, nm in enumerate(directories):\n",
    "    dataset_chunks_dir = Path(f\"chunks_{nm}\")\n",
    "    split_files_for_training(\n",
    "        files_paths=files_paths[i],\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=dataset_chunks_dir,\n",
    "        max_seq_len=block_size+1,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "ds_trn = DatasetMIDI(\n",
    "    files_paths=list(Path('chunks_train').glob(\"**/*.midi\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=block_size+1,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "\n",
    "ds_val = DatasetMIDI(\n",
    "    files_paths=list(Path('chunks_test').glob(\"**/*.midi\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=block_size+1,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=False)\n",
    "dl_trn = DataLoader(ds_trn, batch_size=bs, collate_fn=collator, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=bs * 10, collate_fn=collator, shuffle=True)\n",
    "\n",
    "len(ds_trn), len(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413123b-3d60-4f31-8efb-0fccb45aa2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[304, 276, 278,  37, 370,  53, 235, 176, 305, 329,  56, 401, 287, 241,\n",
       "           56, 224, 182, 242, 298,  66, 278,  27, 230,  39, 241,  63, 450, 250,\n",
       "          224, 186, 276, 278,  41],\n",
       "         [332, 264, 418, 312, 339, 181, 269, 299, 184, 265, 229,  56, 354, 189,\n",
       "          325, 368,  57, 102, 117, 194, 285, 266, 197, 263, 418, 198, 265, 306,\n",
       "          203, 293, 357, 246, 259],\n",
       "         [273, 255, 381, 178, 258, 323, 182, 247,  99, 128, 186, 261, 324, 189,\n",
       "          248, 319, 193, 256,  99, 122, 270,  95, 122, 194, 325,  97, 123, 198,\n",
       "          255, 471, 202, 258, 306],\n",
       "         [246, 240, 355,  39, 495, 253, 224, 177, 260, 244,  44, 229,  55, 343,\n",
       "          179, 261, 486, 237, 480, 251, 292,  53, 468, 184, 238, 235, 185, 253,\n",
       "          277, 187, 275, 236, 189],\n",
       "         [273, 261, 363,  54, 300, 177, 240, 363,  47, 313, 181, 257, 322, 264,\n",
       "          323, 182, 243, 299, 185, 261, 363,  54, 300, 188, 251, 363,  56, 334,\n",
       "          191, 243, 322, 264, 343],\n",
       "         [304, 275, 432, 258, 307,  43, 445, 261, 236, 181, 372, 345, 183, 256,\n",
       "          378, 184, 237, 224, 187, 290, 226,  48, 399, 267, 300, 190, 258, 236,\n",
       "          192, 275, 235, 194, 260],\n",
       "         [273, 257, 226,  56, 231,  49, 227, 176, 242, 226,  54, 429, 252, 244,\n",
       "           46, 231,  34, 227, 181, 358, 281, 183, 257, 268,  58, 229,  49, 414,\n",
       "          263, 225,  63, 225,  51],\n",
       "         [246, 245, 249, 174, 237, 495, 260, 239, 177, 234, 229,  46, 441, 256,\n",
       "          462, 234, 280,  58, 445, 256, 239, 181, 254, 224, 182, 294, 268,  58,\n",
       "          435, 297, 277, 184, 361],\n",
       "         [304, 317, 289,  60, 279,  63, 227, 178, 216,  10, 229,  22, 472, 336,\n",
       "          268,  61, 280,  66, 280,  42, 450, 283, 491, 237, 225,  34, 224, 188,\n",
       "          317, 328, 189, 259, 360],\n",
       "         [273, 242, 229,  65, 239, 177, 297, 298,  62, 467, 258, 306, 179, 253,\n",
       "           98, 118, 180, 276, 244,  58, 470, 271, 298,  53, 266, 185, 258, 455,\n",
       "          260, 356, 189, 237, 229],\n",
       "         [332, 251, 310,  42, 359, 177, 261, 396,  35, 396,  39,  95, 111, 184,\n",
       "          251, 350,  45, 416,  42, 416,  39, 351, 185, 269,  94, 114, 194, 255,\n",
       "           99, 118, 195, 251, 393],\n",
       "         [220, 181, 237, 379, 248, 363,  58, 365,  61, 302, 187, 247, 321, 191,\n",
       "          243, 321, 195, 237, 321, 200, 252, 322, 250, 363,  58, 341,  61, 281,\n",
       "          204, 253, 356, 394, 265],\n",
       "         [273, 260, 330,  34, 289,  52, 466, 237, 462, 253, 330,  56, 289,  34,\n",
       "          266, 181, 248, 450, 238, 307,  53, 478, 267, 249, 185, 237, 236, 188,\n",
       "          252, 307,  34, 341,  57],\n",
       "         [246, 276, 334, 174, 297, 102, 119, 175, 384, 292,  38, 346, 179, 253,\n",
       "          233, 180, 216,  79, 104, 112, 181, 294, 266, 185, 331, 340,  29, 306,\n",
       "          187, 276, 404, 297, 399],\n",
       "         [332, 294, 445, 294, 249, 195, 261, 101, 141, 325,  98, 137, 260, 100,\n",
       "          133, 196, 265,  98, 132, 220, 194, 251, 100, 129, 260,  98, 118, 195,\n",
       "          275,  96, 118, 312,  97],\n",
       "         [273, 262, 322, 178, 257, 315, 179, 269, 365,  52, 337, 243, 324, 184,\n",
       "          285, 249, 189, 293, 302, 190, 305, 316, 195, 274, 313, 200, 263, 313,\n",
       "          201, 262, 314, 240, 357]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_trn[0]cab_sizeb_val1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa751f17-7d25-4d5d-a927-1c788585efd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.k = nn.Linear(n_embed, head_size)\n",
    "        self.q = nn.Linear(n_embed, head_size)\n",
    "        self.v = nn.Linear(n_embed, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.k(x) # B, T, head_size\n",
    "        q = self.q(x) # B, T, head_size\n",
    "        v = self.k(x) # B, T, head_size\n",
    "        \n",
    "        wei = k @ q.transpose(-2, -1) * self.head_size ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = self.dropout(wei.softmax(-1))\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embed // num_heads \n",
    "        self.heads = nn.ModuleList([Head(self.head_size) for i in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.proj(torch.cat([h(x) for h in self.heads], dim=-1))\n",
    "        return self.dropout(out)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.m_head = MultiHeadAttention(n_heads)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.m_head(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class MusicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embs = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embs = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(4),\n",
    "            Block(4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        token_embs = self.token_embs(x) # B, T, C\n",
    "        pos_embs = self.pos_embs(torch.arange(T).to(device)) # T, C\n",
    "\n",
    "        x = token_embs + pos_embs\n",
    "        x = self.blocks(x)\n",
    "        out= self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            B, T, C = out.shape\n",
    "            out = out.view(B * T, C)\n",
    "            targets = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(out, targets)\n",
    "            return out, loss\n",
    "            \n",
    "        return out, None\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for i in range(max_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            out, loss = self(idx_cond)\n",
    "            \n",
    "            out = out[:,-1,:]\n",
    "            probs = out.softmax(-1)\n",
    "            preds = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, preds), dim=-1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdfd83ba-e981-4765-a101-a49b2ee5a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be897e02-a571-43b7-a8ff-b306a0730bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dl_trn):\n\u001b[1;32m      5\u001b[0m         ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m         xb \u001b[38;5;241m=\u001b[39m ids[:, :block_size]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/miditok/pytorch_data/collators.py:99\u001b[0m, in \u001b[0;36mDataCollator.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Pad inputs / convert to Tensors\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43m_pad_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_on_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_dec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     x_dec \u001b[38;5;241m=\u001b[39m _pad_batch(x_dec, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_on_left)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/miditok/pytorch_data/collators.py:163\u001b[0m, in \u001b[0;36m_pad_batch\u001b[0;34m(batch, pad_token_id, pad_on_left)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pad_batch\u001b[39m(\n\u001b[1;32m    149\u001b[0m     batch: Sequence[LongTensor],\n\u001b[1;32m    150\u001b[0m     pad_token_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    151\u001b[0m     pad_on_left: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LongTensor:\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Pad sequences of a batch.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    :return: the batch sequences, padded into a unique Tensor.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     length_of_first \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Check if padding is necessary.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     are_tensors_same_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m length_of_first \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "opt = optim.AdamW(model.parameters(), lr=0.002)\n",
    "n_epochs = 4\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dl_trn):\n",
    "        ids = batch['input_ids']\n",
    "        xb = ids[:, :block_size].to(device)\n",
    "        yb = ids[:, 1:].to(device)\n",
    "        out, loss = model(xb, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            if i % int(len(dl_trn)/10) == 0 or i == len(dl_trn) -1:\n",
    "                print(f'trn: {loss.item()}', end=' ')\n",
    "                val_losses = []\n",
    "                for batch in dl_val:\n",
    "                    ids = batch['input_ids']\n",
    "                    xb = ids[:, :block_size].to(device)\n",
    "                    yb = ids[:, 1:].to(device)\n",
    "                    out_val, loss_val = model(xb, yb)\n",
    "                    val_losses.append(loss_val)\n",
    "                print(f'val: {tensor(val_losses).mean()}')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbcd7a-9b38-4d2e-bdc6-d784d1d8b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    tokenizer(model.generate(torch.ones((1, 1), dtype=torch.long, device=device), max_tokens=300)[0]).dump_midi(f\"recordings/bruh{i}.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8462b8-78df-47b4-84fa-3b59576e52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tokenizer(model.generate(torch.ones((1, 1), dtype=torch.long), max_tokens=3000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fa7a964f-bcfe-4660-90cf-72f587bc2001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(ttype=Tick, tpq=8, begin=0, end=77, tracks=1, notes=5, time_sig=1, key_sig=0, markers=0)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = tokenizer(model.generate(torch.ones((1, 1), dtype=torch.long), max_tokens=30)[0])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "248b041c-ec20-404c-84fe-b5f4637cce7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokSequence(tokens=['Bar_None', 'Bar_None', 'Position_5', 'Program_0', 'Pitch_66', 'Velocity_55', 'Duration_5.0.4', 'Program_0', 'Pitch_62', 'Velocity_23', 'Duration_2.5.8', 'Position_6', 'Program_0', 'Pitch_61', 'Velocity_47', 'Duration_1.1.8', 'Position_9', 'Program_0', 'Pitch_66', 'Velocity_31', 'Duration_0.1.8', 'Program_0', 'Pitch_59', 'Velocity_39', 'Duration_0.4.8'], ids=[4, 4, 178, 280, 50, 99, 144, 280, 46, 95, 129, 179, 280, 45, 98, 117, 182, 280, 50, 96, 109, 280, 43, 97, 112], bytes='', events=[Event(type=Bar, value=None, time=0, desc=0), Event(type=Bar, value=None, time=32, desc=0), Event(type=Position, value=5, time=37, desc=37), Event(type=Program, value=0, time=37, desc=77), Event(type=Pitch, value=66, time=37, desc=77), Event(type=Velocity, value=55, time=37, desc=55), Event(type=Duration, value=5.0.4, time=37, desc=40 ticks), Event(type=Program, value=0, time=37, desc=58), Event(type=Pitch, value=62, time=37, desc=58), Event(type=Velocity, value=23, time=37, desc=23), Event(type=Duration, value=2.5.8, time=37, desc=21 ticks), Event(type=Position, value=6, time=38, desc=38), Event(type=Program, value=0, time=38, desc=47), Event(type=Pitch, value=61, time=38, desc=47), Event(type=Velocity, value=47, time=38, desc=47), Event(type=Duration, value=1.1.8, time=38, desc=9 ticks), Event(type=Position, value=9, time=41, desc=41), Event(type=Program, value=0, time=41, desc=42), Event(type=Pitch, value=66, time=41, desc=42), Event(type=Velocity, value=31, time=41, desc=31), Event(type=Duration, value=0.1.8, time=41, desc=1 ticks), Event(type=Program, value=0, time=41, desc=45), Event(type=Pitch, value=59, time=41, desc=45), Event(type=Velocity, value=39, time=41, desc=39), Event(type=Duration, value=0.4.8, time=41, desc=4 ticks)], are_ids_encoded=False, _ticks_bars=[0, 32], _ticks_beats=[0, 8, 16, 24, 32, 40], _ids_decoded=[])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3ec1f2aa-ec6a-45d5-aca6-3b9e85a11893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(ttype=Tick, tpq=8, begin=0, end=306, tracks=2, notes=2, time_sig=1, key_sig=0, markers=0)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symusic import Score\n",
    "\n",
    "Score('bruh.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "84c9b5e9-ce2a-49fe-b496-5a218706b9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 174, 398, 72, 104, 149, 4, 4, 4, 4, 4, 4, 4, 179, 378, 11, 102, 162]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(Score('bruh.mid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333eadc-96ae-4c54-91ad-f8a5f05d2503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
