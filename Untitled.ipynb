{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6331f45e-9ae5-429f-b0d4-2a9610da8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fdf0fc-1e1e-4bfb-bcee-618934aff025",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 32\n",
    "block_size = 16\n",
    "bs=2\n",
    "vocab_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504fd50a-c01a-4056-a0e1-8f1855b50d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.9/site-packages/miditok/tokenizations/remi.py:77: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Splitting music files (chunks): 100%|██████████| 5/5 [00:00<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[313, 223, 265, 169, 254, 219, 170, 250, 350, 172, 242, 287, 174, 271,\n",
      "          93, 112, 176],\n",
      "        [285, 236, 217,  71, 355,  28, 217,  16, 210,  68, 215, 168, 273, 227,\n",
      "         171, 302, 431]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       dtype=torch.int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Creating a multitrack tokenizer, read the doc to explore all the parameters\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "files_paths = list(Path(\"/notebooks/classical-music-gen/midis_test\").glob(\"**/*.midi\"))\n",
    "tokenizer.train(vocab_size=vocab_size, files_paths=files_paths)\n",
    "tokenizer.save(Path(\"tokenizer\", \"tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "tokenizer.push_to_hub(\"ABicGrill/miditok_tokenizer\", private=True, token=\"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\")\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "dataset_chunks_dir = Path(\"chunks\")\n",
    "split_files_for_training(\n",
    "    files_paths=files_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    save_dir=dataset_chunks_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(dataset_chunks_dir.glob(\"**/*.midi\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=block_size+1,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=False)\n",
    "dataloader = DataLoader(dataset, batch_size=bs, collate_fn=collator)\n",
    "\n",
    "# Iterate over the dataloader to train a model\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa751f17-7d25-4d5d-a927-1c788585efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 32\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.k = nn.Linear(n_embed, head_size)\n",
    "        self.q = nn.Linear(n_embed, head_size)\n",
    "        self.v = nn.Linear(n_embed, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.k(x) # B, T, head_size\n",
    "        q = self.q(x) # B, T, head_size\n",
    "        v = self.k(x) # B, T, head_size\n",
    "        \n",
    "        wei = k @ q.transpose(-2, -1) * self.head_size ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = wei.softmax(-1)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "        \n",
    "class MusicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embs = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embs = nn.Embedding(block_size, n_embed)\n",
    "        self.head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, x, targets=None, train=False):\n",
    "        xb = x[:,:len(x[0])-1] if train else x\n",
    "        if train:\n",
    "            targets = x[:,1:len(x[0])]\n",
    "        token_embs = self.token_embs(xb) # B, T, C\n",
    "        pos_embs = self.pos_embs(torch.arange(len(x[0])-1)) if train else self.pos_embs(torch.arange(len(x[0]))) # T, C\n",
    "\n",
    "        x = token_embs + pos_embs\n",
    "        x = self.head(x)\n",
    "        out= self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            B, T, C = out.shape\n",
    "            out = out.view(B * T, C)\n",
    "            targets = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(out, targets)\n",
    "            return out, loss\n",
    "            \n",
    "        return out, None\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for i in range(max_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            out, loss = self(idx_cond)\n",
    "            out = out[:,-1,:]\n",
    "            probs = out.softmax(-1)\n",
    "            preds = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, preds), dim=-1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfd83ba-e981-4765-a101-a49b2ee5a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d96acf3d-b489-45b9-b52e-4462c5503180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0551, -0.4080,  0.4622,  ..., -0.5808,  1.1027, -0.1700],\n",
       "         [ 0.0690, -0.2885,  0.4294,  ..., -0.4132,  0.8041,  0.0054],\n",
       "         [ 0.2931, -0.1585,  0.2932,  ...,  0.0552,  0.1912,  0.2538],\n",
       "         ...,\n",
       "         [ 0.2675,  0.1726,  0.0611,  ...,  0.0640,  0.0796, -0.1878],\n",
       "         [ 0.4842, -0.2243, -0.0027,  ..., -0.0271, -0.2844,  0.1868],\n",
       "         [ 0.1485, -0.0588,  0.1964,  ..., -0.1663,  0.0137,  0.0048]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(6.2268, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch['input_ids'].shape)\n",
    "model(batch['input_ids'], train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be897e02-a571-43b7-a8ff-b306a0730bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0977, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6310, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "opt = optim.AdamW(model.parameters(), lr=0.002)\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in dataloader:\n",
    "        out, loss = model(batch['input_ids'], train=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        # print(loss.item())\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7cbcd7a-9b38-4d2e-bdc6-d784d1d8b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(model.generate(torch.ones((1, 1), dtype=torch.long), max_tokens=3000)[0]).dump_midi(\"bruh.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a037d1-8400-45de-830b-cb27d21622ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
